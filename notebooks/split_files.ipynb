{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from os.path import join, basename,exists\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cl_args(cl_args):\n",
    "\n",
    "\n",
    "    if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "        sys.argv=sys.argv[:1]\n",
    "\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    for k,v in cl_args.iteritems():\n",
    "        if type(v) is not bool:\n",
    "            parser.add_argument('--' + k, type=type(v), default=v, help=k)\n",
    "        else:\n",
    "            parser.add_argument('--' + k,default=v, action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    cl_args.update(args.__dict__)\n",
    "    return cl_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_list_of_every_file(basepath):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for dir_ in os.listdir(basepath):\n",
    "        ims = get_sorted_list_in_dir(join(basepath,dir_, \"images\"))\n",
    "        images.extend(ims)\n",
    "        lbls = get_sorted_list_in_dir(join(basepath,dir_, \"labels\"))\n",
    "        labels.extend(lbls)\n",
    "    file_pairs = zip(images,labels) \n",
    "    check_matching(pairs=file_pairs)\n",
    "\n",
    "    return file_pairs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_matching(pairs):\n",
    "    for im,lbl in pairs:\n",
    "        im_name = basename(im).split(\".nc\")[0]\n",
    "        lbl_name = basename(lbl).split(\"_label.nc\")[0]\n",
    "        if im_name != lbl_name:\n",
    "            assert False, \"HEY! The pairing is wrong: %s does not match %s\"%(im,lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_files(files,seed):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(files)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sorted_list_in_dir(dir_):\n",
    "    files = os.listdir(dir_)\n",
    "    files.sort()\n",
    "    files = [join(dir_,file_) for file_ in files]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_val_test(file_list, dist={\"tr\":0.8, \"val\":0.1, \"test\": 0.1}):\n",
    "    num_files = len(file_list)\n",
    "    ind = 0\n",
    "    tr_start, val_start, test_start = 0,int(dist[\"tr\"]*num_files), int( (dist[\"tr\"]+ dist[\"val\"]) * num_files)\n",
    "    tr_files, val_files, test_files = file_list[tr_start:val_start], file_list[val_start: test_start], file_list[test_start:]\n",
    "    return dict(zip([\"tr\", \"val\", \"test\"],[tr_files, val_files, test_files]))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pairs(pair_list,path, file_prefix):\n",
    "    if not exists(path):\n",
    "        os.mkdir(path)\n",
    "    f_im = open(join(path, file_prefix + \"_images_list.txt\"), \"w\")\n",
    "    f_lbl = open(join(path, file_prefix + \"_labels_list.txt\"), \"w\")\n",
    "    for im, lbl in pair_list:\n",
    "        im = im.replace(\"\\n\",\"\")\n",
    "        lbl = lbl.replace(\"\\n\", \"\")\n",
    "        f_im.write(im + \"\\n\" )\n",
    "        f_lbl.write(lbl + \"\\n\")\n",
    "    f_im.close()\n",
    "    f_lbl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_pairs_with_crop_indices(pair_list, path, file_prefix, num_crops):\n",
    "    new_pair_list = []\n",
    "    crop_list = []\n",
    "    for pair in pair_list:\n",
    "        for crop in range(num_crops):\n",
    "            new_pair_list.append(pair)\n",
    "            crop_list.append(str(crop))\n",
    "    save_pairs(pair_list=new_pair_list,path=path, file_prefix=file_prefix)\n",
    "    save_crops(crop_list, path, file_prefix=file_prefix)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_crops(crop_list, path, file_prefix):\n",
    "    if not exists(path):\n",
    "        os.mkdir(path)\n",
    "    with open(join(path, file_prefix + \"_crops_list.txt\"), \"w\") as f_crop:\n",
    "        for crop in crop_list:\n",
    "            f_crop.write(crop+ \"\\n\")\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_pairs_from_txt_files(im_txt_file, lbl_txt_file):\n",
    "    ims = open(im_txt_file,\"r\").readlines()\n",
    "    lbls = open(lbl_txt_file,\"r\").readlines()\n",
    "    file_pairs = zip(ims,lbls)\n",
    "    check_matching(file_pairs)\n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_filelist_dir(dest_dir,n, master_path):\n",
    "#     dataset_name = basename(master_path)\n",
    "#     dir_ = join(dest_dir,dataset_name,str(n),\"file_list_files\")\n",
    "    return dest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prototxt_dir(dest_dir,n, master_path):\n",
    "#     dataset_name = basename(master_path)\n",
    "#     dest_dir = join(dest_dir,dataset_name,str(n),\"prototxt_files\")\n",
    "    return dest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_n_file_lists(im_txt_file, lbl_txt_file, n, dest_dir, num_crops, master_file_path, typ=\"tr\", constant_num_for_each=None):\n",
    "    dest_dir = make_filelist_dir(dest_dir, n, master_file_path)\n",
    "    if not exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    print n\n",
    "    def make_n_file_pairs_lists(im_txt_file, lbl_txt_file, n):\n",
    "        file_pairs = get_file_pairs_from_txt_files(im_txt_file=im_txt_file, lbl_txt_file=lbl_txt_file)\n",
    "        num_files = len(file_pairs)\n",
    "        if constant_num_for_each:\n",
    "            num_for_each = constant_num_for_each\n",
    "        else:\n",
    "            num_for_each = num_files / n\n",
    "        remainder = num_files % n\n",
    "        tot = 0\n",
    "        for ind in range(n):\n",
    "            if ind < remainder and constant_num_for_each is None:\n",
    "                \n",
    "                fp = file_pairs[tot: tot + num_for_each + 1]\n",
    "                tot += num_for_each + 1\n",
    "            else:\n",
    "                fp = file_pairs[tot: tot + num_for_each]\n",
    "                tot += num_for_each\n",
    "            yield fp\n",
    "            \n",
    "    \n",
    "    for i, file_pairs in enumerate(make_n_file_pairs_lists(im_txt_file, lbl_txt_file, n)):\n",
    "        if num_crops > 0:\n",
    "            save_pairs_with_crop_indices(file_pairs, dest_dir, file_prefix=\"node_\" + str(i+1) + \"_\" + typ , num_crops=num_crops)\n",
    "        else:\n",
    "            save_pairs(file_pairs,dest_dir,file_prefix=\"node_\" + str(i+1) + \"_\" + typ)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_n_model_and_solver_prototxts(model_prototxt_path, solver_prototxt_path, n, file_list_dir, prototxt_dest_dir, master_path):\n",
    "    prototxt_dest_dir = make_prototxt_dir(prototxt_dest_dir,n, master_path)\n",
    "    if not exists(prototxt_dest_dir):\n",
    "        os.makedirs(prototxt_dest_dir)\n",
    "    for i in range(n):\n",
    "    \n",
    "        prefix=\"node_\" + str(i+1)\n",
    "        im_file = join(file_list_dir,prefix + \"_tr\" + \"_images_list.txt\")\n",
    "        lbl_file = join(file_list_dir,prefix + \"_tr\" + \"_labels_list.txt\")\n",
    "        crops_file = join(file_list_dir,prefix + \"_tr\" + \"_crops_list.txt\")\n",
    "        tr_files = [im_file,lbl_file,crops_file]\n",
    "        \n",
    "        val_im_file = join(file_list_dir,prefix + \"_val\" + \"_images_list.txt\")\n",
    "        val_lbl_file = join(file_list_dir,prefix + \"_val\" + \"_labels_list.txt\")\n",
    "        val_crops_file = join(file_list_dir,prefix + \"_val\" + \"_crops_list.txt\") \n",
    "        val_files = [val_im_file,val_lbl_file,val_crops_file]\n",
    "        new_model_prototxt_path = make_new_model_prototxt(model_prototxt_path,tr_files, val_files, prototxt_dest_dir, prefix=prefix)\n",
    "        make_new_solver_prototxt(new_model_prototxt_path, solver_prototxt_path, prototxt_dest_dir, prefix=prefix)\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_new_solver_prototxt(new_model_prototxt_path, solver_prototxt_path, prototxt_dest_dir, prefix):\n",
    "    new_solver_prototxt_path = join(prototxt_dest_dir, prefix + \"_\" + basename(solver_prototxt_path))\n",
    "    shutil.copy(solver_prototxt_path, new_solver_prototxt_path)\n",
    "    replace_solver_with_new_model_prototxt(new_solver_prototxt_path, new_model_prototxt_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_new_model_prototxt(model_prototxt_path,tr_files, val_files, prototxt_dest_dir, prefix):\n",
    "    new_model_prototxt_path = join(prototxt_dest_dir, prefix + \"_\" + basename(model_prototxt_path))\n",
    "    \n",
    "    shutil.copy(model_prototxt_path, new_model_prototxt_path)\n",
    "    replace_im_label_source_in_prototxt(new_model_prototxt_path, tr_files, val_files)\n",
    "    return new_model_prototxt_path\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_solver_with_new_model_prototxt(solver_prototxt, model_prototxt):\n",
    "    with open(solver_prototxt, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"train_net\" in line or \"test_net\" in line:\n",
    "                lines[i] = lines[i].split('\"')[0] + '\"' + model_prototxt + '\"' + \"\\n\"\n",
    "    with open(solver_prototxt, \"w\") as f:\n",
    "        f.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_im_label_source_in_prototxt(prototxt_path, tr_files, val_files):\n",
    "    tr_im_file,tr_lbl_file,tr_crops_file = tr_files\n",
    "    val_im_file,val_lbl_file,val_crops_file = val_files\n",
    "    with open(prototxt_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if \"source: \" in line:\n",
    "                if \"crop\" not in line:\n",
    "                    if \"image\" in line:\n",
    "                        if \"TRAIN\" in lines[i-3]:\n",
    "                            lines[i] = lines[i].split('\"')[0] + '\"' + tr_im_file + '\"' + \"\\n\"\n",
    "                        else:\n",
    "                            lines[i] = lines[i].split('\"')[0] + '\"' + val_im_file + '\"' + \"\\n\"\n",
    "                    else:\n",
    "                        if \"TRAIN\" in lines[i-3]:\n",
    "                            lines[i] = lines[i].split('\"')[0] + '\"' + tr_lbl_file + '\"' + \"\\n\"\n",
    "                        else:\n",
    "                            lines[i] = lines[i].split('\"')[0] + '\"' + val_lbl_file + '\"' + \"\\n\"\n",
    "                else:\n",
    "                    \n",
    "                        if \"TEST\" in lines[i-14] or \"TEST\" in lines[i-24]:\n",
    "                            lines[i] = lines[i].split('\"')[0] + '\"' + val_crops_file + '\"' + \"\\n\"\n",
    "                        else:\n",
    "                            lines[i] = lines[i].split('\"')[0] + '\"' + tr_crops_file + '\"' + \"\\n\"\n",
    "    with open(prototxt_path, \"w\") as f:\n",
    "        f.writelines(lines)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cl_args = { \"num_nodes\": 32,\n",
    "            \"path_to_files\": \"/global/cscratch1/sd/racah/climate_data/\",\n",
    "            \"master_file_path\": \"/project/projectdirs/dasrepo/gordon_bell/deep_learning/networks/climate/2d_semi_sup/master_file_lists/small_medium/\",\n",
    "            \"files_dest_dir\": \"/project/projectdirs/dasrepo/gordon_bell/deep_learning/networks/climate/2d_semi_sup/multinode_runs/weak_test\",\n",
    "            \"prototxt_dest_dir\": \"/project/projectdirs/dasrepo/gordon_bell/deep_learning/networks/climate/2d_semi_sup/multinode_runs/\",\n",
    "            \"seed\": 3,\n",
    "           \"split_tr_val_test\":False,\n",
    "           \"prototxt_split\": True,\n",
    "           \"file_split\": True,\n",
    "           \"num_crops\": 1,\n",
    "           \"solver_filepath\":\"/project/projectdirs/dasrepo/gordon_bell/deep_learning/networks/climate/2d_semi_sup/base_prototxt_files/solver_vanilla.prototxt\",\n",
    "           \"model_filepath\":\"/project/projectdirs/dasrepo/gordon_bell/deep_learning/networks/climate/2d_semi_sup/base_prototxt_files/train_vanilla.prototxt\", \n",
    "          \"num_total_files\": -1,\n",
    "              \"constant_num_for_each\": 4}\n",
    "    cla = get_cl_args(cl_args)\n",
    "    if cla[\"split_tr_val_test\"]:\n",
    "        file_pairs = get_list_of_every_file(cla['path_to_files'])\n",
    "        file_pairs = shuffle_files(file_pairs,seed=cla[\"seed\"])\n",
    "        if cl_args[\"num_total_files\"] != -1:\n",
    "            file_pairs = file_pairs[:cl_args[\"num_total_files\"]]\n",
    "        file_pairs_dict = split_train_val_test(file_pairs)\n",
    "        for typ, pairs in file_pairs_dict.iteritems():\n",
    "            save_pairs(pairs, cla[\"master_file_path\"],file_prefix=typ)\n",
    "            \n",
    "    \n",
    "    if cla[\"file_split\"]:\n",
    "        \n",
    "        mp = cla[\"master_file_path\"]\n",
    "        im_txt_file, lbl_txt_file = join(mp,\"tr_images_list.txt\"), join(mp,\"tr_labels_list.txt\")\n",
    "        save_n_file_lists(im_txt_file=im_txt_file, \n",
    "                          lbl_txt_file=lbl_txt_file, \n",
    "                          n=cla[\"num_nodes\"], \n",
    "                          dest_dir=cla[\"files_dest_dir\"],\n",
    "                         num_crops=cla[\"num_crops\"],master_file_path=cla[\"master_file_path\"], typ=\"tr\", constant_num_for_each=cla[\"constant_num_for_each\"])\n",
    "        \n",
    "        im_txt_file, lbl_txt_file = join(mp,\"val_images_list.txt\"), join(mp,\"val_labels_list.txt\")\n",
    "        save_n_file_lists(im_txt_file=im_txt_file, \n",
    "                          lbl_txt_file=lbl_txt_file, \n",
    "                          n=cla[\"num_nodes\"], \n",
    "                          dest_dir=cla[\"files_dest_dir\"],\n",
    "                         num_crops=cla[\"num_crops\"],master_file_path=cla[\"master_file_path\"],typ=\"val\", constant_num_for_each=cla[\"constant_num_for_each\"])\n",
    "        \n",
    "        \n",
    "    if cla[\"prototxt_split\"]:\n",
    "        save_n_model_and_solver_prototxts(cla[\"model_filepath\"], \n",
    "                                          cla[\"solver_filepath\"], \n",
    "                                          cla[\"num_nodes\"], \n",
    "                                          make_filelist_dir(cla[\"files_dest_dir\"], cla[\"num_nodes\"], cla[\"master_file_path\"]),\n",
    "                                          cla[\"prototxt_dest_dir\"],cla[\"master_file_path\"])\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook split_files.ipynb to script\n",
      "[NbConvertApp] Writing 12340 bytes to split_files.py\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to script split_files.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mv split_files.py /project/projectdirs/dasrepo/gordon_bell/deep_learning/networks/climate/2d_semi_sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2.7",
   "language": "python",
   "name": "python2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
